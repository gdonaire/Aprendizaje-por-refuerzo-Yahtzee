INFO:train_a2c_vecnorm:Creando 8 entornos paralelos con VecNormalize...
INFO:train_a2c_vecnorm:Activando wrapper de reparacion de acciones invalidas durante TRAINING...
INFO:train_a2c_vecnorm:Aplicando VecNormalize (obs y reward normalizados)...
INFO:train_a2c_vecnorm:Entrenando A2C Fase 1 por 400000 timesteps...
Argumentos de entrenamiento:
  timesteps: 400000
  timesteps2: 400000
  timesteps3: 300000
  n_envs: 8
  seed: 42
  lr: 0.0003
  n_steps: 64
  gamma: 0.995
  ent_coef: 0.02
  gae_lambda: 0.95
  repair_actions_training: True
  eval_episodes: 50
  best_dir: best/a2c_ablation/a2c_baseline/seed_42/best
  plot_ma_long: 200
Using cpu device
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 34.5     |
|    ep_rew_mean        | 95       |
| time/                 |          |
|    fps                | 1587     |
|    iterations         | 100      |
|    time_elapsed       | 32       |
|    total_timesteps    | 51200    |
| train/                |          |
|    entropy_loss       | -3.67    |
|    explained_variance | 0.735    |
|    learning_rate      | 0.0003   |
|    n_updates          | 99       |
|    policy_loss        | 0.32     |
|    value_loss         | 0.289    |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 35.4     |
|    ep_rew_mean        | 93.3     |
| time/                 |          |
|    fps                | 1641     |
|    iterations         | 200      |
|    time_elapsed       | 62       |
|    total_timesteps    | 102400   |
| train/                |          |
|    entropy_loss       | -3.6     |
|    explained_variance | 0.835    |
|    learning_rate      | 0.0003   |
|    n_updates          | 199      |
|    policy_loss        | -0.0146  |
|    value_loss         | 0.197    |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 36.2     |
|    ep_rew_mean        | 102      |
| time/                 |          |
|    fps                | 1641     |
|    iterations         | 300      |
|    time_elapsed       | 93       |
|    total_timesteps    | 153600   |
| train/                |          |
|    entropy_loss       | -3.58    |
|    explained_variance | 0.898    |
|    learning_rate      | 0.0003   |
|    n_updates          | 299      |
|    policy_loss        | 0.167    |
|    value_loss         | 0.125    |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 36.4     |
|    ep_rew_mean        | 102      |
| time/                 |          |
|    fps                | 1641     |
|    iterations         | 400      |
|    time_elapsed       | 124      |
|    total_timesteps    | 204800   |
| train/                |          |
|    entropy_loss       | -3.6     |
|    explained_variance | 0.86     |
|    learning_rate      | 0.0003   |
|    n_updates          | 399      |
|    policy_loss        | -0.363   |
|    value_loss         | 0.177    |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 37       |
|    ep_rew_mean        | 104      |
| time/                 |          |
|    fps                | 1653     |
|    iterations         | 500      |
|    time_elapsed       | 154      |
|    total_timesteps    | 256000   |
| train/                |          |
|    entropy_loss       | -3.6     |
|    explained_variance | 0.87     |
|    learning_rate      | 0.0003   |
|    n_updates          | 499      |
|    policy_loss        | 0.11     |
|    value_loss         | 0.17     |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 37.6     |
|    ep_rew_mean        | 106      |
| time/                 |          |
|    fps                | 1666     |
|    iterations         | 600      |
|    time_elapsed       | 184      |
|    total_timesteps    | 307200   |
| train/                |          |
|    entropy_loss       | -3.55    |
|    explained_variance | 0.869    |
|    learning_rate      | 0.0003   |
|    n_updates          | 599      |
|    policy_loss        | 0.105    |
|    value_loss         | 0.183    |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 37.9     |
|    ep_rew_mean        | 104      |
| time/                 |          |
|    fps                | 1674     |
|    iterations         | 700      |
|    time_elapsed       | 214      |
|    total_timesteps    | 358400   |
| train/                |          |
|    entropy_loss       | -3.55    |
|    explained_variance | 0.812    |
|    learning_rate      | 0.0003   |
|    n_updates          | 699      |
|    policy_loss        | 0.0326   |
|    value_loss         | 0.21     |
------------------------------------
INFO:train_a2c_vecnorm:Entrenando A2C Fase 2 por 400000 timesteps...
Using cpu device
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 38.3     |
|    ep_rew_mean        | 114      |
| time/                 |          |
|    fps                | 1786     |
|    iterations         | 100      |
|    time_elapsed       | 57       |
|    total_timesteps    | 102400   |
| train/                |          |
|    entropy_loss       | -3.55    |
|    explained_variance | 0.881    |
|    learning_rate      | 0.0003   |
|    n_updates          | 99       |
|    policy_loss        | 0.0196   |
|    value_loss         | 0.15     |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 38.1     |
|    ep_rew_mean        | 112      |
| time/                 |          |
|    fps                | 1712     |
|    iterations         | 200      |
|    time_elapsed       | 119      |
|    total_timesteps    | 204800   |
| train/                |          |
|    entropy_loss       | -3.52    |
|    explained_variance | 0.868    |
|    learning_rate      | 0.0003   |
|    n_updates          | 199      |
|    policy_loss        | -0.0967  |
|    value_loss         | 0.164    |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 38.4     |
|    ep_rew_mean        | 113      |
| time/                 |          |
|    fps                | 1693     |
|    iterations         | 300      |
|    time_elapsed       | 181      |
|    total_timesteps    | 307200   |
| train/                |          |
|    entropy_loss       | -3.52    |
|    explained_variance | 0.87     |
|    learning_rate      | 0.0003   |
|    n_updates          | 299      |
|    policy_loss        | 0.131    |
|    value_loss         | 0.197    |
------------------------------------
INFO:train_a2c_vecnorm:Entrenando A2C Fase 3 por 300000 timesteps...
Using cpu device
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 38.7     |
|    ep_rew_mean        | 106      |
| time/                 |          |
|    fps                | 1494     |
|    iterations         | 100      |
|    time_elapsed       | 102      |
|    total_timesteps    | 153600   |
| train/                |          |
|    entropy_loss       | -3.46    |
|    explained_variance | 0.904    |
|    learning_rate      | 0.0003   |
|    n_updates          | 99       |
|    policy_loss        | -0.106   |
|    value_loss         | 0.113    |
------------------------------------
INFO:train_a2c_vecnorm:Modelo guardado en best/a2c_ablation/a2c_baseline/seed_42/best/a2c_yahtzee_vecnorm_final y estadisticas de normalizacion en best/a2c_ablation/a2c_baseline/seed_42/best/a2c_vecnorm_stats.pkl
INFO:final_evaluate_rl:Autodeteccion: modelo=best/a2c_ablation/a2c_baseline/seed_42/best/a2c_yahtzee_vecnorm_final, stats=best/a2c_ablation/a2c_baseline/seed_42/best/a2c_vecnorm_stats.pkl, algo=a2c
INFO:final_evaluate_rl:Cargando modelo: best/a2c_ablation/a2c_baseline/seed_42/best/a2c_yahtzee_vecnorm_final
INFO:final_evaluate_rl:Algo detectado/forzado: a2c
INFO:final_evaluate_rl:Evaluando el modelo con recompensas SIN normalizar(evaluacion segura)...
INFO:final_evaluate_rl:Episode 1: Recompensa del episodio: 111.2420009886846, Puntuacion final: 113.0
INFO:final_evaluate_rl:Episode 2: Recompensa del episodio: 113.82700000330806, Puntuacion final: 117.0
INFO:final_evaluate_rl:Episode 3: Recompensa del episodio: 58.20600017067045, Puntuacion final: 60.0
INFO:final_evaluate_rl:Episode 4: Recompensa del episodio: 131.86299895314733, Puntuacion final: 134.0
INFO:final_evaluate_rl:Episode 5: Recompensa del episodio: 158.8265004956629, Puntuacion final: 160.0
INFO:final_evaluate_rl:Episode 6: Recompensa del episodio: 104.54699886706658, Puntuacion final: 107.0
INFO:final_evaluate_rl:Episode 7: Recompensa del episodio: 89.8769990754081, Puntuacion final: 93.0
INFO:final_evaluate_rl:Episode 8: Recompensa del episodio: 105.2295005741762, Puntuacion final: 107.0
INFO:final_evaluate_rl:Episode 9: Recompensa del episodio: 83.95600001804996, Puntuacion final: 86.0
INFO:final_evaluate_rl:Episode 10: Recompensa del episodio: 144.44399917579722, Puntuacion final: 146.0
INFO:final_evaluate_rl:Episode 11: Recompensa del episodio: 161.32699964928906, Puntuacion final: 164.0
INFO:final_evaluate_rl:Episode 12: Recompensa del episodio: 71.86949973419541, Puntuacion final: 75.0
INFO:final_evaluate_rl:Episode 13: Recompensa del episodio: 99.17149957315996, Puntuacion final: 101.0
INFO:final_evaluate_rl:Episode 14: Recompensa del episodio: 139.30500049726106, Puntuacion final: 142.0
INFO:final_evaluate_rl:Episode 15: Recompensa del episodio: 132.8279988131253, Puntuacion final: 135.0
INFO:final_evaluate_rl:Episode 16: Recompensa del episodio: 66.43850012961775, Puntuacion final: 69.0
INFO:final_evaluate_rl:Episode 17: Recompensa del episodio: 80.27949990483467, Puntuacion final: 82.0
INFO:final_evaluate_rl:Episode 18: Recompensa del episodio: 93.44699935778044, Puntuacion final: 95.0
INFO:final_evaluate_rl:Episode 19: Recompensa del episodio: 152.70949920319254, Puntuacion final: 155.0
INFO:final_evaluate_rl:Episode 20: Recompensa del episodio: 100.43650100275408, Puntuacion final: 103.0
INFO:final_evaluate_rl:Episode 21: Recompensa del episodio: 100.53699985495768, Puntuacion final: 102.0
INFO:final_evaluate_rl:Episode 22: Recompensa del episodio: 101.99499833583832, Puntuacion final: 104.0
INFO:final_evaluate_rl:Episode 23: Recompensa del episodio: 87.33400208607782, Puntuacion final: 89.0
INFO:final_evaluate_rl:Episode 24: Recompensa del episodio: 96.74500033142976, Puntuacion final: 99.0
INFO:final_evaluate_rl:Episode 25: Recompensa del episodio: 137.32349980907748, Puntuacion final: 140.0
INFO:final_evaluate_rl:Episode 26: Recompensa del episodio: 118.72400218399707, Puntuacion final: 121.0
INFO:final_evaluate_rl:Episode 27: Recompensa del episodio: 160.83349966292735, Puntuacion final: 163.0
INFO:final_evaluate_rl:Episode 28: Recompensa del episodio: 133.55300113989506, Puntuacion final: 135.0
INFO:final_evaluate_rl:Episode 29: Recompensa del episodio: 114.31349842157215, Puntuacion final: 117.0
INFO:final_evaluate_rl:Episode 30: Recompensa del episodio: 106.68199877941515, Puntuacion final: 109.0
INFO:final_evaluate_rl:Episode 31: Recompensa del episodio: 93.08749879291281, Puntuacion final: 95.0
INFO:final_evaluate_rl:Episode 32: Recompensa del episodio: 98.85050067584962, Puntuacion final: 101.0
INFO:final_evaluate_rl:Episode 33: Recompensa del episodio: 125.55099906481337, Puntuacion final: 127.0
INFO:final_evaluate_rl:Episode 34: Recompensa del episodio: 144.83399770094547, Puntuacion final: 146.0
INFO:final_evaluate_rl:Episode 35: Recompensa del episodio: 142.7685006794054, Puntuacion final: 146.0
INFO:final_evaluate_rl:Episode 36: Recompensa del episodio: 134.33799936552532, Puntuacion final: 137.0
INFO:final_evaluate_rl:Episode 37: Recompensa del episodio: 90.72350042173639, Puntuacion final: 93.0
INFO:final_evaluate_rl:Episode 38: Recompensa del episodio: 159.59149910369888, Puntuacion final: 162.0
INFO:final_evaluate_rl:Episode 39: Recompensa del episodio: 127.04200000001583, Puntuacion final: 129.0
INFO:final_evaluate_rl:Episode 40: Recompensa del episodio: 162.90249852789566, Puntuacion final: 165.0
INFO:final_evaluate_rl:Episode 41: Recompensa del episodio: 118.74649936682545, Puntuacion final: 121.0
INFO:final_evaluate_rl:Episode 42: Recompensa del episodio: 115.01550122897606, Puntuacion final: 117.0
INFO:final_evaluate_rl:Episode 43: Recompensa del episodio: 161.03399768611416, Puntuacion final: 163.0
INFO:final_evaluate_rl:Episode 44: Recompensa del episodio: 102.86950065323617, Puntuacion final: 105.0
INFO:final_evaluate_rl:Episode 45: Recompensa del episodio: 162.82250217092223, Puntuacion final: 167.0
INFO:final_evaluate_rl:Episode 46: Recompensa del episodio: 142.8549995003268, Puntuacion final: 145.0
INFO:final_evaluate_rl:Episode 47: Recompensa del episodio: 70.79250090220012, Puntuacion final: 72.0
INFO:final_evaluate_rl:Episode 48: Recompensa del episodio: 116.44900054112077, Puntuacion final: 118.0
INFO:final_evaluate_rl:Episode 49: Recompensa del episodio: 102.04049954819493, Puntuacion final: 104.0
INFO:final_evaluate_rl:Episode 50: Recompensa del episodio: 117.84499831171706, Puntuacion final: 121.0
INFO:final_evaluate_rl:Metricas guardadas en a2c_vecnorm_final_evaluation_metrics.txt
Grafica guardada en best/a2c_ablation/a2c_baseline/seed_42/best/a2c_vecnorm_reward_evolution.png

Resumen:
  Recompensa media (sin normalizar): 116.9606
  Puntuacion final promedio:        119.1400
