INFO:train_a2c_vecnorm:Creando 8 entornos paralelos con VecNormalize...
INFO:train_a2c_vecnorm:Activando wrapper de reparacion de acciones invalidas durante TRAINING...
INFO:train_a2c_vecnorm:Aplicando VecNormalize (obs y reward normalizados)...
INFO:train_a2c_vecnorm:Entrenando A2C Fase 1 por 400000 timesteps...
Argumentos de entrenamiento:
  timesteps: 400000
  timesteps2: 400000
  timesteps3: 300000
  n_envs: 8
  seed: 126
  lr: 0.0003
  n_steps: 64
  gamma: 0.995
  ent_coef: 0.02
  gae_lambda: 0.95
  repair_actions_training: True
  eval_episodes: 50
  best_dir: best/a2c_ablation/a2c_baseline/seed_126/best
  plot_ma_long: 200
Using cpu device
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 34.4     |
|    ep_rew_mean        | 90.1     |
| time/                 |          |
|    fps                | 1932     |
|    iterations         | 100      |
|    time_elapsed       | 26       |
|    total_timesteps    | 51200    |
| train/                |          |
|    entropy_loss       | -3.72    |
|    explained_variance | 0.843    |
|    learning_rate      | 0.0003   |
|    n_updates          | 99       |
|    policy_loss        | -0.247   |
|    value_loss         | 0.176    |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 35.2     |
|    ep_rew_mean        | 95       |
| time/                 |          |
|    fps                | 1953     |
|    iterations         | 200      |
|    time_elapsed       | 52       |
|    total_timesteps    | 102400   |
| train/                |          |
|    entropy_loss       | -3.64    |
|    explained_variance | 0.839    |
|    learning_rate      | 0.0003   |
|    n_updates          | 199      |
|    policy_loss        | 0.299    |
|    value_loss         | 0.215    |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 35.8     |
|    ep_rew_mean        | 99.6     |
| time/                 |          |
|    fps                | 1962     |
|    iterations         | 300      |
|    time_elapsed       | 78       |
|    total_timesteps    | 153600   |
| train/                |          |
|    entropy_loss       | -3.63    |
|    explained_variance | 0.879    |
|    learning_rate      | 0.0003   |
|    n_updates          | 299      |
|    policy_loss        | 0.175    |
|    value_loss         | 0.169    |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 36.5     |
|    ep_rew_mean        | 104      |
| time/                 |          |
|    fps                | 1963     |
|    iterations         | 400      |
|    time_elapsed       | 104      |
|    total_timesteps    | 204800   |
| train/                |          |
|    entropy_loss       | -3.63    |
|    explained_variance | 0.902    |
|    learning_rate      | 0.0003   |
|    n_updates          | 399      |
|    policy_loss        | -0.358   |
|    value_loss         | 0.12     |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 36.9     |
|    ep_rew_mean        | 98.3     |
| time/                 |          |
|    fps                | 1956     |
|    iterations         | 500      |
|    time_elapsed       | 130      |
|    total_timesteps    | 256000   |
| train/                |          |
|    entropy_loss       | -3.6     |
|    explained_variance | 0.831    |
|    learning_rate      | 0.0003   |
|    n_updates          | 499      |
|    policy_loss        | 0.0895   |
|    value_loss         | 0.213    |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 37.5     |
|    ep_rew_mean        | 103      |
| time/                 |          |
|    fps                | 1958     |
|    iterations         | 600      |
|    time_elapsed       | 156      |
|    total_timesteps    | 307200   |
| train/                |          |
|    entropy_loss       | -3.6     |
|    explained_variance | 0.813    |
|    learning_rate      | 0.0003   |
|    n_updates          | 599      |
|    policy_loss        | -0.0696  |
|    value_loss         | 0.227    |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 37.8     |
|    ep_rew_mean        | 107      |
| time/                 |          |
|    fps                | 1957     |
|    iterations         | 700      |
|    time_elapsed       | 183      |
|    total_timesteps    | 358400   |
| train/                |          |
|    entropy_loss       | -3.58    |
|    explained_variance | 0.889    |
|    learning_rate      | 0.0003   |
|    n_updates          | 699      |
|    policy_loss        | 0.019    |
|    value_loss         | 0.145    |
------------------------------------
INFO:train_a2c_vecnorm:Entrenando A2C Fase 2 por 400000 timesteps...
Using cpu device
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 38.4     |
|    ep_rew_mean        | 109      |
| time/                 |          |
|    fps                | 1984     |
|    iterations         | 100      |
|    time_elapsed       | 51       |
|    total_timesteps    | 102400   |
| train/                |          |
|    entropy_loss       | -3.55    |
|    explained_variance | 0.855    |
|    learning_rate      | 0.0003   |
|    n_updates          | 99       |
|    policy_loss        | 0.348    |
|    value_loss         | 0.194    |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 38.5     |
|    ep_rew_mean        | 109      |
| time/                 |          |
|    fps                | 1981     |
|    iterations         | 200      |
|    time_elapsed       | 103      |
|    total_timesteps    | 204800   |
| train/                |          |
|    entropy_loss       | -3.51    |
|    explained_variance | 0.878    |
|    learning_rate      | 0.0003   |
|    n_updates          | 199      |
|    policy_loss        | 0.00671  |
|    value_loss         | 0.153    |
------------------------------------
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 38.5     |
|    ep_rew_mean        | 110      |
| time/                 |          |
|    fps                | 1986     |
|    iterations         | 300      |
|    time_elapsed       | 154      |
|    total_timesteps    | 307200   |
| train/                |          |
|    entropy_loss       | -3.52    |
|    explained_variance | 0.914    |
|    learning_rate      | 0.0003   |
|    n_updates          | 299      |
|    policy_loss        | -0.293   |
|    value_loss         | 0.116    |
------------------------------------
INFO:train_a2c_vecnorm:Entrenando A2C Fase 3 por 300000 timesteps...
Using cpu device
------------------------------------
| rollout/              |          |
|    ep_len_mean        | 38.5     |
|    ep_rew_mean        | 108      |
| time/                 |          |
|    fps                | 1982     |
|    iterations         | 100      |
|    time_elapsed       | 77       |
|    total_timesteps    | 153600   |
| train/                |          |
|    entropy_loss       | -3.47    |
|    explained_variance | 0.849    |
|    learning_rate      | 0.0003   |
|    n_updates          | 99       |
|    policy_loss        | 0.0969   |
|    value_loss         | 0.195    |
------------------------------------
INFO:train_a2c_vecnorm:Modelo guardado en best/a2c_ablation/a2c_baseline/seed_126/best/a2c_yahtzee_vecnorm_final y estadisticas de normalizacion en best/a2c_ablation/a2c_baseline/seed_126/best/a2c_vecnorm_stats.pkl
INFO:final_evaluate_rl:Autodeteccion: modelo=best/a2c_ablation/a2c_baseline/seed_126/best/a2c_yahtzee_vecnorm_final, stats=best/a2c_ablation/a2c_baseline/seed_126/best/a2c_vecnorm_stats.pkl, algo=a2c
INFO:final_evaluate_rl:Cargando modelo: best/a2c_ablation/a2c_baseline/seed_126/best/a2c_yahtzee_vecnorm_final
INFO:final_evaluate_rl:Algo detectado/forzado: a2c
INFO:final_evaluate_rl:Evaluando el modelo con recompensas SIN normalizar(evaluacion segura)...
INFO:final_evaluate_rl:Episode 1: Recompensa del episodio: 178.3164987473283, Puntuacion final: 180.0
INFO:final_evaluate_rl:Episode 2: Recompensa del episodio: 144.03149810439209, Puntuacion final: 146.0
INFO:final_evaluate_rl:Episode 3: Recompensa del episodio: 110.9989992469782, Puntuacion final: 113.0
INFO:final_evaluate_rl:Episode 4: Recompensa del episodio: 84.92349993670359, Puntuacion final: 87.0
INFO:final_evaluate_rl:Episode 5: Recompensa del episodio: 86.96949917962775, Puntuacion final: 89.0
INFO:final_evaluate_rl:Episode 6: Recompensa del episodio: 107.55300022789743, Puntuacion final: 109.0
INFO:final_evaluate_rl:Episode 7: Recompensa del episodio: 132.98250100482255, Puntuacion final: 135.0
INFO:final_evaluate_rl:Episode 8: Recompensa del episodio: 113.41550073504914, Puntuacion final: 115.0
INFO:final_evaluate_rl:Episode 9: Recompensa del episodio: 133.35100024275016, Puntuacion final: 136.0
INFO:final_evaluate_rl:Episode 10: Recompensa del episodio: 136.8144993088208, Puntuacion final: 139.0
INFO:final_evaluate_rl:Episode 11: Recompensa del episodio: 125.9255009200424, Puntuacion final: 128.0
INFO:final_evaluate_rl:Episode 12: Recompensa del episodio: 79.06049840827473, Puntuacion final: 81.0
INFO:final_evaluate_rl:Episode 13: Recompensa del episodio: 104.44699994812254, Puntuacion final: 106.0
INFO:final_evaluate_rl:Episode 14: Recompensa del episodio: 91.2659989669919, Puntuacion final: 93.0
INFO:final_evaluate_rl:Episode 15: Recompensa del episodio: 79.8595005357638, Puntuacion final: 83.0
INFO:final_evaluate_rl:Episode 16: Recompensa del episodio: 116.8399990326725, Puntuacion final: 120.0
INFO:final_evaluate_rl:Episode 17: Recompensa del episodio: 119.461500746198, Puntuacion final: 121.0
INFO:final_evaluate_rl:Episode 18: Recompensa del episodio: 95.48299971880624, Puntuacion final: 97.0
INFO:final_evaluate_rl:Episode 19: Recompensa del episodio: 162.83949957415462, Puntuacion final: 165.0
INFO:final_evaluate_rl:Episode 20: Recompensa del episodio: 114.121500454552, Puntuacion final: 116.0
INFO:final_evaluate_rl:Episode 21: Recompensa del episodio: 170.80699776113033, Puntuacion final: 175.0
INFO:final_evaluate_rl:Episode 22: Recompensa del episodio: 153.1205013525905, Puntuacion final: 155.0
INFO:final_evaluate_rl:Episode 23: Recompensa del episodio: 76.50950144452509, Puntuacion final: 79.0
INFO:final_evaluate_rl:Episode 24: Recompensa del episodio: 109.1634992081672, Puntuacion final: 111.0
INFO:final_evaluate_rl:Episode 25: Recompensa del episodio: 85.34050039993599, Puntuacion final: 88.0
INFO:final_evaluate_rl:Episode 26: Recompensa del episodio: 146.2424994854373, Puntuacion final: 148.0
INFO:final_evaluate_rl:Episode 27: Recompensa del episodio: 165.58149959612638, Puntuacion final: 167.0
INFO:final_evaluate_rl:Episode 28: Recompensa del episodio: 153.1219995932188, Puntuacion final: 155.0
INFO:final_evaluate_rl:Episode 29: Recompensa del episodio: 155.60200036596507, Puntuacion final: 158.0
INFO:final_evaluate_rl:Episode 30: Recompensa del episodio: 114.64999944949523, Puntuacion final: 117.0
INFO:final_evaluate_rl:Episode 31: Recompensa del episodio: 139.0490003565792, Puntuacion final: 141.0
INFO:final_evaluate_rl:Episode 32: Recompensa del episodio: 109.05499766254798, Puntuacion final: 111.0
INFO:final_evaluate_rl:Episode 33: Recompensa del episodio: 101.07149850158021, Puntuacion final: 103.0
INFO:final_evaluate_rl:Episode 34: Recompensa del episodio: 162.69800144457258, Puntuacion final: 165.0
INFO:final_evaluate_rl:Episode 35: Recompensa del episodio: 166.79699947708286, Puntuacion final: 170.0
INFO:final_evaluate_rl:Episode 36: Recompensa del episodio: 132.63250117283314, Puntuacion final: 135.0
INFO:final_evaluate_rl:Episode 37: Recompensa del episodio: 122.82399897137657, Puntuacion final: 124.0
INFO:final_evaluate_rl:Episode 38: Recompensa del episodio: 92.87600128632039, Puntuacion final: 97.0
INFO:final_evaluate_rl:Episode 39: Recompensa del episodio: 151.80650047550444, Puntuacion final: 156.0
INFO:final_evaluate_rl:Episode 40: Recompensa del episodio: 94.68299908086192, Puntuacion final: 96.0
INFO:final_evaluate_rl:Episode 41: Recompensa del episodio: 89.26749867480248, Puntuacion final: 91.0
INFO:final_evaluate_rl:Episode 42: Recompensa del episodio: 157.9055002387613, Puntuacion final: 160.0
INFO:final_evaluate_rl:Episode 43: Recompensa del episodio: 145.82999765133718, Puntuacion final: 149.0
INFO:final_evaluate_rl:Episode 44: Recompensa del episodio: 169.2155011803843, Puntuacion final: 172.0
INFO:final_evaluate_rl:Episode 45: Recompensa del episodio: 82.13699970929883, Puntuacion final: 84.0
INFO:final_evaluate_rl:Episode 46: Recompensa del episodio: 88.92999812174821, Puntuacion final: 91.0
INFO:final_evaluate_rl:Episode 47: Recompensa del episodio: 107.8690012494335, Puntuacion final: 112.0
INFO:final_evaluate_rl:Episode 48: Recompensa del episodio: 95.86050120485015, Puntuacion final: 98.0
INFO:final_evaluate_rl:Episode 49: Recompensa del episodio: 122.16699971864, Puntuacion final: 124.0
INFO:final_evaluate_rl:Episode 50: Recompensa del episodio: 85.0705009746016, Puntuacion final: 87.0
INFO:final_evaluate_rl:Metricas guardadas en a2c_vecnorm_final_evaluation_metrics.txt
Grafica guardada en best/a2c_ablation/a2c_baseline/seed_126/best/a2c_vecnorm_reward_evolution.png

Resumen:
  Recompensa media (sin normalizar): 121.3309
  Puntuacion final promedio:        123.5600
