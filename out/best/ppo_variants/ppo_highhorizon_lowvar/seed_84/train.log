INFO:train_ppo_vecnorm:Creando 8 entornos paralelos con VecNormalize...
INFO:train_ppo_vecnorm:Activando wrapper de reparacion de acciones invalidas durante TRAINING...
INFO:train_ppo_vecnorm:Aplicando VecNormalize (obs y reward normalizados)...
INFO:train_ppo_vecnorm:Entrenando PPO Fase 1 por 400000 timesteps...
Argumentos de entrenamiento:
  timesteps: 400000
  timesteps2: 400000
  timesteps3: 300000
  n_envs: 8
  seed: 84
  lr: 0.0003
  n_steps: 4096
  batch_size: 128
  n_epochs: 10
  gamma: 0.995
  gae_lambda: 0.97
  ent_coef: 0.02
  clip_range: 0.2
  vf_coef: 0.5
  max_grad_norm: 0.5
  target_kl: None
  use_sde: False
  sde_sample_freq: -1
  policy_arch: {"pi":[256, 256], "vf":[256, 256]}
  eval_episodes: 50
  best_dir: best/ppo_ablation/ppo_highhorizon_lowvar/seed_84/best
  plot_ma_long: 200
  repair_actions_training: True
Using cpu device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.5     |
|    ep_rew_mean     | 83.1     |
| time/              |          |
|    fps             | 2090     |
|    iterations      | 1        |
|    time_elapsed    | 15       |
|    total_timesteps | 32768    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 33.9       |
|    ep_rew_mean          | 86.1       |
| time/                   |            |
|    fps                  | 1369       |
|    iterations           | 2          |
|    time_elapsed         | 47         |
|    total_timesteps      | 65536      |
| train/                  |            |
|    approx_kl            | 0.04028385 |
|    clip_fraction        | 0.195      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.75      |
|    explained_variance   | -0.323     |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0173     |
|    n_updates            | 10         |
|    policy_gradient_loss | -0.018     |
|    value_loss           | 0.281      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 34.3        |
|    ep_rew_mean          | 87.5        |
| time/                   |             |
|    fps                  | 1217        |
|    iterations           | 3           |
|    time_elapsed         | 80          |
|    total_timesteps      | 98304       |
| train/                  |             |
|    approx_kl            | 0.014170963 |
|    clip_fraction        | 0.197       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.71       |
|    explained_variance   | 0.694       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0245      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0162     |
|    value_loss           | 0.243       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 34.8       |
|    ep_rew_mean          | 90.9       |
| time/                   |            |
|    fps                  | 1155       |
|    iterations           | 4          |
|    time_elapsed         | 113        |
|    total_timesteps      | 131072     |
| train/                  |            |
|    approx_kl            | 0.01423388 |
|    clip_fraction        | 0.205      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.69      |
|    explained_variance   | 0.755      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00804    |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.0177    |
|    value_loss           | 0.245      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 35.2       |
|    ep_rew_mean          | 94.4       |
| time/                   |            |
|    fps                  | 1120       |
|    iterations           | 5          |
|    time_elapsed         | 146        |
|    total_timesteps      | 163840     |
| train/                  |            |
|    approx_kl            | 0.01473226 |
|    clip_fraction        | 0.221      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.67      |
|    explained_variance   | 0.774      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.000951  |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.022     |
|    value_loss           | 0.237      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 35.5        |
|    ep_rew_mean          | 98.5        |
| time/                   |             |
|    fps                  | 1097        |
|    iterations           | 6           |
|    time_elapsed         | 179         |
|    total_timesteps      | 196608      |
| train/                  |             |
|    approx_kl            | 0.014579985 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.65       |
|    explained_variance   | 0.79        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00117    |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0214     |
|    value_loss           | 0.217       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 35.8       |
|    ep_rew_mean          | 98.3       |
| time/                   |            |
|    fps                  | 1082       |
|    iterations           | 7          |
|    time_elapsed         | 211        |
|    total_timesteps      | 229376     |
| train/                  |            |
|    approx_kl            | 0.01476465 |
|    clip_fraction        | 0.216      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.63      |
|    explained_variance   | 0.786      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0169    |
|    n_updates            | 60         |
|    policy_gradient_loss | -0.0246    |
|    value_loss           | 0.23       |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 36.4        |
|    ep_rew_mean          | 101         |
| time/                   |             |
|    fps                  | 1067        |
|    iterations           | 8           |
|    time_elapsed         | 245         |
|    total_timesteps      | 262144      |
| train/                  |             |
|    approx_kl            | 0.015940685 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.61       |
|    explained_variance   | 0.793       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0278     |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0271     |
|    value_loss           | 0.223       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 36.4        |
|    ep_rew_mean          | 104         |
| time/                   |             |
|    fps                  | 1057        |
|    iterations           | 9           |
|    time_elapsed         | 278         |
|    total_timesteps      | 294912      |
| train/                  |             |
|    approx_kl            | 0.016529648 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.59       |
|    explained_variance   | 0.805       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.013      |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0286     |
|    value_loss           | 0.203       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 36.5        |
|    ep_rew_mean          | 101         |
| time/                   |             |
|    fps                  | 1052        |
|    iterations           | 10          |
|    time_elapsed         | 311         |
|    total_timesteps      | 327680      |
| train/                  |             |
|    approx_kl            | 0.017776128 |
|    clip_fraction        | 0.237       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.57       |
|    explained_variance   | 0.804       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0109     |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.031      |
|    value_loss           | 0.215       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 36.8        |
|    ep_rew_mean          | 98.5        |
| time/                   |             |
|    fps                  | 1046        |
|    iterations           | 11          |
|    time_elapsed         | 344         |
|    total_timesteps      | 360448      |
| train/                  |             |
|    approx_kl            | 0.018617397 |
|    clip_fraction        | 0.245       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.55       |
|    explained_variance   | 0.808       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0181     |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0332     |
|    value_loss           | 0.209       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 37          |
|    ep_rew_mean          | 112         |
| time/                   |             |
|    fps                  | 1042        |
|    iterations           | 12          |
|    time_elapsed         | 377         |
|    total_timesteps      | 393216      |
| train/                  |             |
|    approx_kl            | 0.020138191 |
|    clip_fraction        | 0.258       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.53       |
|    explained_variance   | 0.809       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0402     |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.0345     |
|    value_loss           | 0.207       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 37          |
|    ep_rew_mean          | 108         |
| time/                   |             |
|    fps                  | 1038        |
|    iterations           | 13          |
|    time_elapsed         | 410         |
|    total_timesteps      | 425984      |
| train/                  |             |
|    approx_kl            | 0.020656358 |
|    clip_fraction        | 0.261       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.51       |
|    explained_variance   | 0.812       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0417     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0352     |
|    value_loss           | 0.197       |
-----------------------------------------
INFO:train_ppo_vecnorm:Entrenando PPO Fase 2 por 400000 timesteps...
Using cpu device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 37.2     |
|    ep_rew_mean     | 107      |
| time/              |          |
|    fps             | 1997     |
|    iterations      | 1        |
|    time_elapsed    | 32       |
|    total_timesteps | 65536    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 37.3       |
|    ep_rew_mean          | 105        |
| time/                   |            |
|    fps                  | 1284       |
|    iterations           | 2          |
|    time_elapsed         | 102        |
|    total_timesteps      | 131072     |
| train/                  |            |
|    approx_kl            | 0.02282536 |
|    clip_fraction        | 0.277      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.47      |
|    explained_variance   | 0.807      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00063   |
|    n_updates            | 10         |
|    policy_gradient_loss | -0.0275    |
|    value_loss           | 0.219      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 37.5       |
|    ep_rew_mean          | 107        |
| time/                   |            |
|    fps                  | 1156       |
|    iterations           | 3          |
|    time_elapsed         | 169        |
|    total_timesteps      | 196608     |
| train/                  |            |
|    approx_kl            | 0.02445753 |
|    clip_fraction        | 0.287      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.44      |
|    explained_variance   | 0.821      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.037      |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.0284    |
|    value_loss           | 0.205      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 37.5        |
|    ep_rew_mean          | 110         |
| time/                   |             |
|    fps                  | 1106        |
|    iterations           | 4           |
|    time_elapsed         | 236         |
|    total_timesteps      | 262144      |
| train/                  |             |
|    approx_kl            | 0.024969967 |
|    clip_fraction        | 0.294       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.42       |
|    explained_variance   | 0.818       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0113      |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0298     |
|    value_loss           | 0.207       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 37.3        |
|    ep_rew_mean          | 110         |
| time/                   |             |
|    fps                  | 1080        |
|    iterations           | 5           |
|    time_elapsed         | 303         |
|    total_timesteps      | 327680      |
| train/                  |             |
|    approx_kl            | 0.025973063 |
|    clip_fraction        | 0.304       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.4        |
|    explained_variance   | 0.818       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0237      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0306     |
|    value_loss           | 0.204       |
-----------------------------------------
---------------------------------------
| rollout/                |           |
|    ep_len_mean          | 37.4      |
|    ep_rew_mean          | 108       |
| time/                   |           |
|    fps                  | 1065      |
|    iterations           | 6         |
|    time_elapsed         | 369       |
|    total_timesteps      | 393216    |
| train/                  |           |
|    approx_kl            | 0.0264947 |
|    clip_fraction        | 0.311     |
|    clip_range           | 0.2       |
|    entropy_loss         | -3.38     |
|    explained_variance   | 0.818     |
|    learning_rate        | 0.0003    |
|    loss                 | 0.014     |
|    n_updates            | 50        |
|    policy_gradient_loss | -0.0317   |
|    value_loss           | 0.203     |
---------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 37.6        |
|    ep_rew_mean          | 109         |
| time/                   |             |
|    fps                  | 1054        |
|    iterations           | 7           |
|    time_elapsed         | 434         |
|    total_timesteps      | 458752      |
| train/                  |             |
|    approx_kl            | 0.028071597 |
|    clip_fraction        | 0.321       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.36       |
|    explained_variance   | 0.831       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00849     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0326     |
|    value_loss           | 0.186       |
-----------------------------------------
INFO:train_ppo_vecnorm:Entrenando PPO Fase 3 por 300000 timesteps...
Using cpu device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 37.8     |
|    ep_rew_mean     | 114      |
| time/              |          |
|    fps             | 2046     |
|    iterations      | 1        |
|    time_elapsed    | 48       |
|    total_timesteps | 98304    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 37.7        |
|    ep_rew_mean          | 112         |
| time/                   |             |
|    fps                  | 1296        |
|    iterations           | 2           |
|    time_elapsed         | 151         |
|    total_timesteps      | 196608      |
| train/                  |             |
|    approx_kl            | 0.029444844 |
|    clip_fraction        | 0.334       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.31       |
|    explained_variance   | 0.829       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0766      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0266     |
|    value_loss           | 0.198       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 37.9        |
|    ep_rew_mean          | 112         |
| time/                   |             |
|    fps                  | 1166        |
|    iterations           | 3           |
|    time_elapsed         | 252         |
|    total_timesteps      | 294912      |
| train/                  |             |
|    approx_kl            | 0.030519241 |
|    clip_fraction        | 0.341       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.28       |
|    explained_variance   | 0.832       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0496      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0268     |
|    value_loss           | 0.194       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 38          |
|    ep_rew_mean          | 111         |
| time/                   |             |
|    fps                  | 1113        |
|    iterations           | 4           |
|    time_elapsed         | 353         |
|    total_timesteps      | 393216      |
| train/                  |             |
|    approx_kl            | 0.030742047 |
|    clip_fraction        | 0.342       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.26       |
|    explained_variance   | 0.836       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.032       |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0276     |
|    value_loss           | 0.191       |
-----------------------------------------
INFO:train_ppo_vecnorm:Modelo guardado en best/ppo_ablation/ppo_highhorizon_lowvar/seed_84/best/ppo_yahtzee_vecnorm_final y estadisticas de normalizacion en best/ppo_ablation/ppo_highhorizon_lowvar/seed_84/best/ppo_vecnorm_stats.pkl
INFO:final_evaluate_rl:Autodeteccion: modelo=best/ppo_ablation/ppo_highhorizon_lowvar/seed_84/best/ppo_yahtzee_vecnorm_final, stats=best/ppo_ablation/ppo_highhorizon_lowvar/seed_84/best/ppo_vecnorm_stats.pkl, algo=ppo
INFO:final_evaluate_rl:Cargando modelo: best/ppo_ablation/ppo_highhorizon_lowvar/seed_84/best/ppo_yahtzee_vecnorm_final
INFO:final_evaluate_rl:Algo detectado/forzado: ppo
INFO:final_evaluate_rl:Evaluando el modelo con recompensas SIN normalizar(evaluacion segura)...
INFO:final_evaluate_rl:Episode 1: Recompensa del episodio: 130.7235005197581, Puntuacion final: 133.0
INFO:final_evaluate_rl:Episode 2: Recompensa del episodio: 182.7180000069784, Puntuacion final: 185.0
INFO:final_evaluate_rl:Episode 3: Recompensa del episodio: 99.4724989363458, Puntuacion final: 101.0
INFO:final_evaluate_rl:Episode 4: Recompensa del episodio: 94.92300034582149, Puntuacion final: 97.0
INFO:final_evaluate_rl:Episode 5: Recompensa del episodio: 125.33399784535868, Puntuacion final: 127.0
INFO:final_evaluate_rl:Episode 6: Recompensa del episodio: 101.71299890591763, Puntuacion final: 103.0
INFO:final_evaluate_rl:Episode 7: Recompensa del episodio: 103.54799956735224, Puntuacion final: 106.0
INFO:final_evaluate_rl:Episode 8: Recompensa del episodio: 137.3489993950352, Puntuacion final: 139.0
INFO:final_evaluate_rl:Episode 9: Recompensa del episodio: 117.53099980775733, Puntuacion final: 120.0
INFO:final_evaluate_rl:Episode 10: Recompensa del episodio: 159.65300025069155, Puntuacion final: 162.0
INFO:final_evaluate_rl:Episode 11: Recompensa del episodio: 77.77699872723315, Puntuacion final: 79.0
INFO:final_evaluate_rl:Episode 12: Recompensa del episodio: 133.70750155299902, Puntuacion final: 136.0
INFO:final_evaluate_rl:Episode 13: Recompensa del episodio: 123.95649950904772, Puntuacion final: 126.0
INFO:final_evaluate_rl:Episode 14: Recompensa del episodio: 100.0360008282587, Puntuacion final: 101.0
INFO:final_evaluate_rl:Episode 15: Recompensa del episodio: 126.86349834653083, Puntuacion final: 131.0
INFO:final_evaluate_rl:Episode 16: Recompensa del episodio: 130.22100027895067, Puntuacion final: 133.0
INFO:final_evaluate_rl:Episode 17: Recompensa del episodio: 141.04950029728934, Puntuacion final: 143.0
INFO:final_evaluate_rl:Episode 18: Recompensa del episodio: 98.5269986288622, Puntuacion final: 100.0
INFO:final_evaluate_rl:Episode 19: Recompensa del episodio: 131.44849922531284, Puntuacion final: 133.0
INFO:final_evaluate_rl:Episode 20: Recompensa del episodio: 165.0594993273262, Puntuacion final: 167.0
INFO:final_evaluate_rl:Episode 21: Recompensa del episodio: 128.54849903122522, Puntuacion final: 131.0
INFO:final_evaluate_rl:Episode 22: Recompensa del episodio: 140.8320015585632, Puntuacion final: 143.0
INFO:final_evaluate_rl:Episode 23: Recompensa del episodio: 71.6634989400045, Puntuacion final: 73.0
INFO:final_evaluate_rl:Episode 24: Recompensa del episodio: 111.94850149581907, Puntuacion final: 114.0
INFO:final_evaluate_rl:Episode 25: Recompensa del episodio: 143.2995017906651, Puntuacion final: 145.0
INFO:final_evaluate_rl:Episode 26: Recompensa del episodio: 160.83399835741147, Puntuacion final: 163.0
INFO:final_evaluate_rl:Episode 27: Recompensa del episodio: 137.58849742688471, Puntuacion final: 140.0
INFO:final_evaluate_rl:Episode 28: Recompensa del episodio: 140.6340009626001, Puntuacion final: 142.0
INFO:final_evaluate_rl:Episode 29: Recompensa del episodio: 139.6929999726126, Puntuacion final: 142.0
INFO:final_evaluate_rl:Episode 30: Recompensa del episodio: 86.38550043920986, Puntuacion final: 89.0
INFO:final_evaluate_rl:Episode 31: Recompensa del episodio: 111.7854994967347, Puntuacion final: 115.0
INFO:final_evaluate_rl:Episode 32: Recompensa del episodio: 108.81699948012829, Puntuacion final: 113.0
INFO:final_evaluate_rl:Episode 33: Recompensa del episodio: 139.6355004137149, Puntuacion final: 142.0
INFO:final_evaluate_rl:Episode 34: Recompensa del episodio: 149.7255001483718, Puntuacion final: 152.0
INFO:final_evaluate_rl:Episode 35: Recompensa del episodio: 160.43849913263693, Puntuacion final: 162.0
INFO:final_evaluate_rl:Episode 36: Recompensa del episodio: 101.87600051576737, Puntuacion final: 105.0
INFO:final_evaluate_rl:Episode 37: Recompensa del episodio: 147.19350166153163, Puntuacion final: 149.0
INFO:final_evaluate_rl:Episode 38: Recompensa del episodio: 125.21549978363328, Puntuacion final: 127.0
INFO:final_evaluate_rl:Episode 39: Recompensa del episodio: 87.87750061653787, Puntuacion final: 91.0
INFO:final_evaluate_rl:Episode 40: Recompensa del episodio: 101.81099891662598, Puntuacion final: 106.0
INFO:final_evaluate_rl:Episode 41: Recompensa del episodio: 110.79299847688526, Puntuacion final: 114.0
INFO:final_evaluate_rl:Episode 42: Recompensa del episodio: 120.13900150853442, Puntuacion final: 122.0
INFO:final_evaluate_rl:Episode 43: Recompensa del episodio: 109.3359997337684, Puntuacion final: 111.0
INFO:final_evaluate_rl:Episode 44: Recompensa del episodio: 119.12199856212828, Puntuacion final: 121.0
INFO:final_evaluate_rl:Episode 45: Recompensa del episodio: 147.80350068130065, Puntuacion final: 151.0
INFO:final_evaluate_rl:Episode 46: Recompensa del episodio: 128.79250144632533, Puntuacion final: 131.0
INFO:final_evaluate_rl:Episode 47: Recompensa del episodio: 130.632000800455, Puntuacion final: 133.0
INFO:final_evaluate_rl:Episode 48: Recompensa del episodio: 84.43300029216334, Puntuacion final: 86.0
INFO:final_evaluate_rl:Episode 49: Recompensa del episodio: 112.85000241175294, Puntuacion final: 117.0
INFO:final_evaluate_rl:Episode 50: Recompensa del episodio: 120.52350035775453, Puntuacion final: 122.0
INFO:final_evaluate_rl:Metricas guardadas en ppo_vecnorm_final_evaluation_metrics.txt
Grafica guardada en best/ppo_ablation/ppo_highhorizon_lowvar/seed_84/best/ppo_vecnorm_reward_evolution.png

Resumen:
  Recompensa media (sin normalizar): 123.2368
  Puntuacion final promedio:        125.4800
