INFO:train_ppo_vecnorm:Creando 8 entornos paralelos con VecNormalize...
INFO:train_ppo_vecnorm:Activando wrapper de reparacion de acciones invalidas durante TRAINING...
INFO:train_ppo_vecnorm:Aplicando VecNormalize (obs y reward normalizados)...
INFO:train_ppo_vecnorm:Entrenando PPO Fase 1 por 400000 timesteps...
Argumentos de entrenamiento:
  timesteps: 400000
  timesteps2: 400000
  timesteps3: 300000
  n_envs: 8
  seed: 126
  lr: 0.0003
  n_steps: 4096
  batch_size: 128
  n_epochs: 10
  gamma: 0.995
  gae_lambda: 0.97
  ent_coef: 0.02
  clip_range: 0.2
  vf_coef: 0.5
  max_grad_norm: 0.5
  target_kl: None
  use_sde: False
  sde_sample_freq: -1
  policy_arch: {"pi":[256, 256], "vf":[256, 256]}
  eval_episodes: 50
  best_dir: best/ppo_ablation/ppo_highhorizon_lowvar/seed_126/best
  plot_ma_long: 200
  repair_actions_training: True
Using cpu device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 33.7     |
|    ep_rew_mean     | 81.1     |
| time/              |          |
|    fps             | 2029     |
|    iterations      | 1        |
|    time_elapsed    | 16       |
|    total_timesteps | 32768    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 33.6       |
|    ep_rew_mean          | 73.7       |
| time/                   |            |
|    fps                  | 1316       |
|    iterations           | 2          |
|    time_elapsed         | 49         |
|    total_timesteps      | 65536      |
| train/                  |            |
|    approx_kl            | 0.01903621 |
|    clip_fraction        | 0.199      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.77      |
|    explained_variance   | 0.147      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00272    |
|    n_updates            | 10         |
|    policy_gradient_loss | -0.0152    |
|    value_loss           | 0.288      |
----------------------------------------
------------------------------------------
| rollout/                |              |
|    ep_len_mean          | 34.3         |
|    ep_rew_mean          | 86.4         |
| time/                   |              |
|    fps                  | 1175         |
|    iterations           | 3            |
|    time_elapsed         | 83           |
|    total_timesteps      | 98304        |
| train/                  |              |
|    approx_kl            | 0.0136839785 |
|    clip_fraction        | 0.194        |
|    clip_range           | 0.2          |
|    entropy_loss         | -3.75        |
|    explained_variance   | 0.68         |
|    learning_rate        | 0.0003       |
|    loss                 | 0.0378       |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.016       |
|    value_loss           | 0.27         |
------------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 35.1        |
|    ep_rew_mean          | 86.8        |
| time/                   |             |
|    fps                  | 1122        |
|    iterations           | 4           |
|    time_elapsed         | 116         |
|    total_timesteps      | 131072      |
| train/                  |             |
|    approx_kl            | 0.013186773 |
|    clip_fraction        | 0.196       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.73       |
|    explained_variance   | 0.749       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00402     |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0179     |
|    value_loss           | 0.247       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 35.3       |
|    ep_rew_mean          | 90.3       |
| time/                   |            |
|    fps                  | 1089       |
|    iterations           | 5          |
|    time_elapsed         | 150        |
|    total_timesteps      | 163840     |
| train/                  |            |
|    approx_kl            | 0.01344868 |
|    clip_fraction        | 0.206      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.7       |
|    explained_variance   | 0.765      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00292    |
|    n_updates            | 40         |
|    policy_gradient_loss | -0.0201    |
|    value_loss           | 0.242      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 35.6        |
|    ep_rew_mean          | 97.3        |
| time/                   |             |
|    fps                  | 1070        |
|    iterations           | 6           |
|    time_elapsed         | 183         |
|    total_timesteps      | 196608      |
| train/                  |             |
|    approx_kl            | 0.013662942 |
|    clip_fraction        | 0.2         |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.69       |
|    explained_variance   | 0.774       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.024      |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0207     |
|    value_loss           | 0.24        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 35.9        |
|    ep_rew_mean          | 96.8        |
| time/                   |             |
|    fps                  | 1054        |
|    iterations           | 7           |
|    time_elapsed         | 217         |
|    total_timesteps      | 229376      |
| train/                  |             |
|    approx_kl            | 0.014497351 |
|    clip_fraction        | 0.209       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.67       |
|    explained_variance   | 0.795       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0342     |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0234     |
|    value_loss           | 0.221       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 36          |
|    ep_rew_mean          | 103         |
| time/                   |             |
|    fps                  | 1039        |
|    iterations           | 8           |
|    time_elapsed         | 252         |
|    total_timesteps      | 262144      |
| train/                  |             |
|    approx_kl            | 0.014885218 |
|    clip_fraction        | 0.208       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.65       |
|    explained_variance   | 0.781       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.032       |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0245     |
|    value_loss           | 0.24        |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 36.5        |
|    ep_rew_mean          | 102         |
| time/                   |             |
|    fps                  | 1031        |
|    iterations           | 9           |
|    time_elapsed         | 286         |
|    total_timesteps      | 294912      |
| train/                  |             |
|    approx_kl            | 0.015964659 |
|    clip_fraction        | 0.221       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.63       |
|    explained_variance   | 0.807       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0144     |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0273     |
|    value_loss           | 0.212       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 36.8       |
|    ep_rew_mean          | 104        |
| time/                   |            |
|    fps                  | 1017       |
|    iterations           | 10         |
|    time_elapsed         | 322        |
|    total_timesteps      | 327680     |
| train/                  |            |
|    approx_kl            | 0.01674054 |
|    clip_fraction        | 0.223      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.61      |
|    explained_variance   | 0.793      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0336    |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0282    |
|    value_loss           | 0.23       |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 36.8        |
|    ep_rew_mean          | 103         |
| time/                   |             |
|    fps                  | 1011        |
|    iterations           | 11          |
|    time_elapsed         | 356         |
|    total_timesteps      | 360448      |
| train/                  |             |
|    approx_kl            | 0.017448962 |
|    clip_fraction        | 0.228       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.59       |
|    explained_variance   | 0.802       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0208     |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.03       |
|    value_loss           | 0.216       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 36.5       |
|    ep_rew_mean          | 110        |
| time/                   |            |
|    fps                  | 1006       |
|    iterations           | 12         |
|    time_elapsed         | 390        |
|    total_timesteps      | 393216     |
| train/                  |            |
|    approx_kl            | 0.01828952 |
|    clip_fraction        | 0.239      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.57      |
|    explained_variance   | 0.806      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0518    |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.0312    |
|    value_loss           | 0.206      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 36.7       |
|    ep_rew_mean          | 107        |
| time/                   |            |
|    fps                  | 1004       |
|    iterations           | 13         |
|    time_elapsed         | 424        |
|    total_timesteps      | 425984     |
| train/                  |            |
|    approx_kl            | 0.01969877 |
|    clip_fraction        | 0.253      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.55      |
|    explained_variance   | 0.813      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.0225    |
|    n_updates            | 120        |
|    policy_gradient_loss | -0.0334    |
|    value_loss           | 0.201      |
----------------------------------------
INFO:train_ppo_vecnorm:Entrenando PPO Fase 2 por 400000 timesteps...
Using cpu device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 36.8     |
|    ep_rew_mean     | 103      |
| time/              |          |
|    fps             | 1334     |
|    iterations      | 1        |
|    time_elapsed    | 49       |
|    total_timesteps | 65536    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 36.9       |
|    ep_rew_mean          | 110        |
| time/                   |            |
|    fps                  | 849        |
|    iterations           | 2          |
|    time_elapsed         | 154        |
|    total_timesteps      | 131072     |
| train/                  |            |
|    approx_kl            | 0.02155285 |
|    clip_fraction        | 0.267      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.51      |
|    explained_variance   | 0.809      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0133     |
|    n_updates            | 10         |
|    policy_gradient_loss | -0.0253    |
|    value_loss           | 0.207      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 37.1       |
|    ep_rew_mean          | 106        |
| time/                   |            |
|    fps                  | 813        |
|    iterations           | 3          |
|    time_elapsed         | 241        |
|    total_timesteps      | 196608     |
| train/                  |            |
|    approx_kl            | 0.02260815 |
|    clip_fraction        | 0.277      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.49      |
|    explained_variance   | 0.812      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0468     |
|    n_updates            | 20         |
|    policy_gradient_loss | -0.0262    |
|    value_loss           | 0.211      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 37.2        |
|    ep_rew_mean          | 105         |
| time/                   |             |
|    fps                  | 806         |
|    iterations           | 4           |
|    time_elapsed         | 325         |
|    total_timesteps      | 262144      |
| train/                  |             |
|    approx_kl            | 0.023109887 |
|    clip_fraction        | 0.281       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.47       |
|    explained_variance   | 0.811       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.037       |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0276     |
|    value_loss           | 0.214       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 37.6        |
|    ep_rew_mean          | 112         |
| time/                   |             |
|    fps                  | 807         |
|    iterations           | 5           |
|    time_elapsed         | 405         |
|    total_timesteps      | 327680      |
| train/                  |             |
|    approx_kl            | 0.024528932 |
|    clip_fraction        | 0.292       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.44       |
|    explained_variance   | 0.824       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0609      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0287     |
|    value_loss           | 0.195       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 37.6        |
|    ep_rew_mean          | 110         |
| time/                   |             |
|    fps                  | 803         |
|    iterations           | 6           |
|    time_elapsed         | 489         |
|    total_timesteps      | 393216      |
| train/                  |             |
|    approx_kl            | 0.025543503 |
|    clip_fraction        | 0.301       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.42       |
|    explained_variance   | 0.815       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00839    |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0298     |
|    value_loss           | 0.203       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 37.1        |
|    ep_rew_mean          | 108         |
| time/                   |             |
|    fps                  | 798         |
|    iterations           | 7           |
|    time_elapsed         | 574         |
|    total_timesteps      | 458752      |
| train/                  |             |
|    approx_kl            | 0.026559459 |
|    clip_fraction        | 0.31        |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.4        |
|    explained_variance   | 0.823       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0316      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0305     |
|    value_loss           | 0.198       |
-----------------------------------------
INFO:train_ppo_vecnorm:Entrenando PPO Fase 3 por 300000 timesteps...
Using cpu device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 37.6     |
|    ep_rew_mean     | 108      |
| time/              |          |
|    fps             | 1583     |
|    iterations      | 1        |
|    time_elapsed    | 62       |
|    total_timesteps | 98304    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 37.8        |
|    ep_rew_mean          | 113         |
| time/                   |             |
|    fps                  | 1015        |
|    iterations           | 2           |
|    time_elapsed         | 193         |
|    total_timesteps      | 196608      |
| train/                  |             |
|    approx_kl            | 0.028680505 |
|    clip_fraction        | 0.328       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.36       |
|    explained_variance   | 0.826       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0266      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0263     |
|    value_loss           | 0.191       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 37.6        |
|    ep_rew_mean          | 117         |
| time/                   |             |
|    fps                  | 918         |
|    iterations           | 3           |
|    time_elapsed         | 320         |
|    total_timesteps      | 294912      |
| train/                  |             |
|    approx_kl            | 0.030005028 |
|    clip_fraction        | 0.335       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.33       |
|    explained_variance   | 0.824       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0105      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0268     |
|    value_loss           | 0.2         |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 37.9       |
|    ep_rew_mean          | 116        |
| time/                   |            |
|    fps                  | 888        |
|    iterations           | 4          |
|    time_elapsed         | 442        |
|    total_timesteps      | 393216     |
| train/                  |            |
|    approx_kl            | 0.03015147 |
|    clip_fraction        | 0.339      |
|    clip_range           | 0.2        |
|    entropy_loss         | -3.3       |
|    explained_variance   | 0.826      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0239     |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.0275    |
|    value_loss           | 0.199      |
----------------------------------------
INFO:train_ppo_vecnorm:Modelo guardado en best/ppo_ablation/ppo_highhorizon_lowvar/seed_126/best/ppo_yahtzee_vecnorm_final y estadisticas de normalizacion en best/ppo_ablation/ppo_highhorizon_lowvar/seed_126/best/ppo_vecnorm_stats.pkl
INFO:final_evaluate_rl:Autodeteccion: modelo=best/ppo_ablation/ppo_highhorizon_lowvar/seed_126/best/ppo_yahtzee_vecnorm_final, stats=best/ppo_ablation/ppo_highhorizon_lowvar/seed_126/best/ppo_vecnorm_stats.pkl, algo=ppo
INFO:final_evaluate_rl:Cargando modelo: best/ppo_ablation/ppo_highhorizon_lowvar/seed_126/best/ppo_yahtzee_vecnorm_final
INFO:final_evaluate_rl:Algo detectado/forzado: ppo
INFO:final_evaluate_rl:Evaluando el modelo con recompensas SIN normalizar(evaluacion segura)...
INFO:final_evaluate_rl:Episode 1: Recompensa del episodio: 161.28399871988222, Puntuacion final: 163.0
INFO:final_evaluate_rl:Episode 2: Recompensa del episodio: 74.91600016923621, Puntuacion final: 77.0
INFO:final_evaluate_rl:Episode 3: Recompensa del episodio: 140.00900118984282, Puntuacion final: 142.0
INFO:final_evaluate_rl:Episode 4: Recompensa del episodio: 132.8274986280594, Puntuacion final: 137.0
INFO:final_evaluate_rl:Episode 5: Recompensa del episodio: 109.86849995848024, Puntuacion final: 112.0
INFO:final_evaluate_rl:Episode 6: Recompensa del episodio: 147.34150003665127, Puntuacion final: 149.0
INFO:final_evaluate_rl:Episode 7: Recompensa del episodio: 169.70499948295765, Puntuacion final: 172.0
INFO:final_evaluate_rl:Episode 8: Recompensa del episodio: 121.12449860770721, Puntuacion final: 123.0
INFO:final_evaluate_rl:Episode 9: Recompensa del episodio: 110.51049912074814, Puntuacion final: 112.0
INFO:final_evaluate_rl:Episode 10: Recompensa del episodio: 158.9594984255964, Puntuacion final: 161.0
INFO:final_evaluate_rl:Episode 11: Recompensa del episodio: 81.77349946368486, Puntuacion final: 83.0
INFO:final_evaluate_rl:Episode 12: Recompensa del episodio: 170.98900129413232, Puntuacion final: 173.0
INFO:final_evaluate_rl:Episode 13: Recompensa del episodio: 126.22249939083122, Puntuacion final: 128.0
INFO:final_evaluate_rl:Episode 14: Recompensa del episodio: 102.04799876734614, Puntuacion final: 104.0
INFO:final_evaluate_rl:Episode 15: Recompensa del episodio: 73.21600015490549, Puntuacion final: 75.0
INFO:final_evaluate_rl:Episode 16: Recompensa del episodio: 154.00250032404438, Puntuacion final: 156.0
INFO:final_evaluate_rl:Episode 17: Recompensa del episodio: 104.05599897203501, Puntuacion final: 106.0
INFO:final_evaluate_rl:Episode 18: Recompensa del episodio: 173.08800339139998, Puntuacion final: 175.0
INFO:final_evaluate_rl:Episode 19: Recompensa del episodio: 80.14549941883888, Puntuacion final: 82.0
INFO:final_evaluate_rl:Episode 20: Recompensa del episodio: 87.58100181177724, Puntuacion final: 89.0
INFO:final_evaluate_rl:Episode 21: Recompensa del episodio: 118.44400035357103, Puntuacion final: 120.0
INFO:final_evaluate_rl:Episode 22: Recompensa del episodio: 106.77549743984127, Puntuacion final: 109.0
INFO:final_evaluate_rl:Episode 23: Recompensa del episodio: 69.82750017847866, Puntuacion final: 73.0
INFO:final_evaluate_rl:Episode 24: Recompensa del episodio: 96.05000130739063, Puntuacion final: 97.0
INFO:final_evaluate_rl:Episode 25: Recompensa del episodio: 99.85599813988665, Puntuacion final: 104.0
INFO:final_evaluate_rl:Episode 26: Recompensa del episodio: 64.57299863430671, Puntuacion final: 66.0
INFO:final_evaluate_rl:Episode 27: Recompensa del episodio: 121.7884989453014, Puntuacion final: 124.0
INFO:final_evaluate_rl:Episode 28: Recompensa del episodio: 99.07349937455729, Puntuacion final: 101.0
INFO:final_evaluate_rl:Episode 29: Recompensa del episodio: 105.23300067521632, Puntuacion final: 108.0
INFO:final_evaluate_rl:Episode 30: Recompensa del episodio: 179.32250169897452, Puntuacion final: 181.0
INFO:final_evaluate_rl:Episode 31: Recompensa del episodio: 108.34200047980994, Puntuacion final: 110.0
INFO:final_evaluate_rl:Episode 32: Recompensa del episodio: 145.92199890036136, Puntuacion final: 148.0
INFO:final_evaluate_rl:Episode 33: Recompensa del episodio: 139.8394979913719, Puntuacion final: 142.0
INFO:final_evaluate_rl:Episode 34: Recompensa del episodio: 103.20099918264896, Puntuacion final: 105.0
INFO:final_evaluate_rl:Episode 35: Recompensa del episodio: 115.83600176370237, Puntuacion final: 119.0
INFO:final_evaluate_rl:Episode 36: Recompensa del episodio: 139.02949848538265, Puntuacion final: 141.0
INFO:final_evaluate_rl:Episode 37: Recompensa del episodio: 84.14400010555983, Puntuacion final: 86.0
INFO:final_evaluate_rl:Episode 38: Recompensa del episodio: 146.0964998434065, Puntuacion final: 148.0
INFO:final_evaluate_rl:Episode 39: Recompensa del episodio: 88.84249907574849, Puntuacion final: 93.0
INFO:final_evaluate_rl:Episode 40: Recompensa del episodio: 161.83799832616933, Puntuacion final: 164.0
INFO:final_evaluate_rl:Episode 41: Recompensa del episodio: 73.05250008380972, Puntuacion final: 75.0
INFO:final_evaluate_rl:Episode 42: Recompensa del episodio: 115.03750050277449, Puntuacion final: 117.0
INFO:final_evaluate_rl:Episode 43: Recompensa del episodio: 99.87349941785214, Puntuacion final: 102.0
INFO:final_evaluate_rl:Episode 44: Recompensa del episodio: 117.837500133086, Puntuacion final: 122.0
INFO:final_evaluate_rl:Episode 45: Recompensa del episodio: 70.13500004645903, Puntuacion final: 72.0
INFO:final_evaluate_rl:Episode 46: Recompensa del episodio: 133.80649924755562, Puntuacion final: 136.0
INFO:final_evaluate_rl:Episode 47: Recompensa del episodio: 129.32200017030118, Puntuacion final: 132.0
INFO:final_evaluate_rl:Episode 48: Recompensa del episodio: 108.94399983761832, Puntuacion final: 111.0
INFO:final_evaluate_rl:Episode 49: Recompensa del episodio: 165.10549931187415, Puntuacion final: 167.0
INFO:final_evaluate_rl:Episode 50: Recompensa del episodio: 152.5924990804633, Puntuacion final: 155.0
INFO:final_evaluate_rl:Metricas guardadas en ppo_vecnorm_final_evaluation_metrics.txt
Grafica guardada en best/ppo_ablation/ppo_highhorizon_lowvar/seed_126/best/ppo_vecnorm_reward_evolution.png

Resumen:
  Recompensa media (sin normalizar): 118.7882
  Puntuacion final promedio:        120.9400
