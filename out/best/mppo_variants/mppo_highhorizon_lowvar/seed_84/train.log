INFO:train_maskablePPO_vecnorm:Creando 8 entornos paralelos con VecNormalize...
INFO:train_maskablePPO_vecnorm:Aplicando VecNormalize (obs y reward normalizados)...
INFO:train_maskablePPO_vecnorm:Usando arquitectura de politica: {'net_arch': {'pi': [256, 256], 'vf': [256, 256]}}
INFO:train_maskablePPO_vecnorm:Entrenando MaskablePPO Fase 1 por 400000 timesteps...
Argumentos de entrenamiento:
  timesteps: 400000
  timesteps2: 400000
  timesteps3: 300000
  n_envs: 8
  seed: 84
  lr: 0.0003
  n_steps: 4096
  batch_size: 128
  n_epochs: 10
  gamma: 0.995
  gae_lambda: 0.97
  ent_coef: 0.02
  clip_range: 0.2
  vf_coef: 0.5
  max_grad_norm: 0.5
  policy_arch: {"pi":[256, 256], "vf":[256, 256]}
  eval_episodes: 50
  best_dir: best/mppo_ablation/mppo_highhorizon_lowvar/seed_84/best
  plot_ma_long: 200
Using cpu device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 32.8     |
|    ep_rew_mean     | 46       |
| time/              |          |
|    fps             | 1946     |
|    iterations      | 1        |
|    time_elapsed    | 16       |
|    total_timesteps | 32768    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 32.5        |
|    ep_rew_mean          | 47.9        |
| time/                   |             |
|    fps                  | 1205        |
|    iterations           | 2           |
|    time_elapsed         | 54          |
|    total_timesteps      | 65536       |
| train/                  |             |
|    approx_kl            | 0.019083815 |
|    clip_fraction        | 0.192       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.05       |
|    explained_variance   | -0.446      |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0819      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0167     |
|    value_loss           | 0.347       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 32.7        |
|    ep_rew_mean          | 49.2        |
| time/                   |             |
|    fps                  | 1076        |
|    iterations           | 3           |
|    time_elapsed         | 91          |
|    total_timesteps      | 98304       |
| train/                  |             |
|    approx_kl            | 0.014454755 |
|    clip_fraction        | 0.206       |
|    clip_range           | 0.2         |
|    entropy_loss         | -3.01       |
|    explained_variance   | 0.574       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.143       |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0189     |
|    value_loss           | 0.289       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 31.9        |
|    ep_rew_mean          | 56.3        |
| time/                   |             |
|    fps                  | 1019        |
|    iterations           | 4           |
|    time_elapsed         | 128         |
|    total_timesteps      | 131072      |
| train/                  |             |
|    approx_kl            | 0.015321982 |
|    clip_fraction        | 0.218       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.99       |
|    explained_variance   | 0.662       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0242      |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0238     |
|    value_loss           | 0.303       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 32.1        |
|    ep_rew_mean          | 65.5        |
| time/                   |             |
|    fps                  | 979         |
|    iterations           | 5           |
|    time_elapsed         | 167         |
|    total_timesteps      | 163840      |
| train/                  |             |
|    approx_kl            | 0.015261281 |
|    clip_fraction        | 0.22        |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.97       |
|    explained_variance   | 0.676       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0456      |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0265     |
|    value_loss           | 0.322       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 32.3        |
|    ep_rew_mean          | 67.8        |
| time/                   |             |
|    fps                  | 960         |
|    iterations           | 6           |
|    time_elapsed         | 204         |
|    total_timesteps      | 196608      |
| train/                  |             |
|    approx_kl            | 0.016583081 |
|    clip_fraction        | 0.225       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.92       |
|    explained_variance   | 0.7         |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0389      |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0294     |
|    value_loss           | 0.324       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 32.2        |
|    ep_rew_mean          | 74.2        |
| time/                   |             |
|    fps                  | 944         |
|    iterations           | 7           |
|    time_elapsed         | 242         |
|    total_timesteps      | 229376      |
| train/                  |             |
|    approx_kl            | 0.017141618 |
|    clip_fraction        | 0.234       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.88       |
|    explained_variance   | 0.738       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0766      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0308     |
|    value_loss           | 0.301       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 31.9        |
|    ep_rew_mean          | 81          |
| time/                   |             |
|    fps                  | 936         |
|    iterations           | 8           |
|    time_elapsed         | 279         |
|    total_timesteps      | 262144      |
| train/                  |             |
|    approx_kl            | 0.018793326 |
|    clip_fraction        | 0.243       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.85       |
|    explained_variance   | 0.753       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0365      |
|    n_updates            | 70          |
|    policy_gradient_loss | -0.0337     |
|    value_loss           | 0.283       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 32.3       |
|    ep_rew_mean          | 87.4       |
| time/                   |            |
|    fps                  | 907        |
|    iterations           | 9          |
|    time_elapsed         | 324        |
|    total_timesteps      | 294912     |
| train/                  |            |
|    approx_kl            | 0.02002833 |
|    clip_fraction        | 0.252      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.81      |
|    explained_variance   | 0.784      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.0188     |
|    n_updates            | 80         |
|    policy_gradient_loss | -0.0349    |
|    value_loss           | 0.251      |
----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 31.8       |
|    ep_rew_mean          | 92.7       |
| time/                   |            |
|    fps                  | 895        |
|    iterations           | 10         |
|    time_elapsed         | 366        |
|    total_timesteps      | 327680     |
| train/                  |            |
|    approx_kl            | 0.02026114 |
|    clip_fraction        | 0.254      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.76      |
|    explained_variance   | 0.815      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00605    |
|    n_updates            | 90         |
|    policy_gradient_loss | -0.0364    |
|    value_loss           | 0.225      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 31.5        |
|    ep_rew_mean          | 98.6        |
| time/                   |             |
|    fps                  | 894         |
|    iterations           | 11          |
|    time_elapsed         | 402         |
|    total_timesteps      | 360448      |
| train/                  |             |
|    approx_kl            | 0.020712994 |
|    clip_fraction        | 0.259       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.72       |
|    explained_variance   | 0.826       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.00417     |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0375     |
|    value_loss           | 0.21        |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 31.8       |
|    ep_rew_mean          | 105        |
| time/                   |            |
|    fps                  | 892        |
|    iterations           | 12         |
|    time_elapsed         | 440        |
|    total_timesteps      | 393216     |
| train/                  |            |
|    approx_kl            | 0.02211599 |
|    clip_fraction        | 0.267      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.68      |
|    explained_variance   | 0.84       |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00616   |
|    n_updates            | 110        |
|    policy_gradient_loss | -0.0377    |
|    value_loss           | 0.195      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 31.6        |
|    ep_rew_mean          | 109         |
| time/                   |             |
|    fps                  | 885         |
|    iterations           | 13          |
|    time_elapsed         | 481         |
|    total_timesteps      | 425984      |
| train/                  |             |
|    approx_kl            | 0.023049232 |
|    clip_fraction        | 0.271       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.64       |
|    explained_variance   | 0.847       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.0427     |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0376     |
|    value_loss           | 0.18        |
-----------------------------------------
INFO:train_maskablePPO_vecnorm:Entrenando MaskablePPO Fase 2 por 400000 timesteps...
Using cpu device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 31.7     |
|    ep_rew_mean     | 118      |
| time/              |          |
|    fps             | 1854     |
|    iterations      | 1        |
|    time_elapsed    | 35       |
|    total_timesteps | 65536    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 31.4        |
|    ep_rew_mean          | 111         |
| time/                   |             |
|    fps                  | 1175        |
|    iterations           | 2           |
|    time_elapsed         | 111         |
|    total_timesteps      | 131072      |
| train/                  |             |
|    approx_kl            | 0.025902286 |
|    clip_fraction        | 0.285       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.55       |
|    explained_variance   | 0.857       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0191      |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0302     |
|    value_loss           | 0.176       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 31.1        |
|    ep_rew_mean          | 122         |
| time/                   |             |
|    fps                  | 1008        |
|    iterations           | 3           |
|    time_elapsed         | 194         |
|    total_timesteps      | 196608      |
| train/                  |             |
|    approx_kl            | 0.027414061 |
|    clip_fraction        | 0.289       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.5        |
|    explained_variance   | 0.862       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0189      |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0315     |
|    value_loss           | 0.163       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 31.2        |
|    ep_rew_mean          | 124         |
| time/                   |             |
|    fps                  | 960         |
|    iterations           | 4           |
|    time_elapsed         | 272         |
|    total_timesteps      | 262144      |
| train/                  |             |
|    approx_kl            | 0.027833803 |
|    clip_fraction        | 0.286       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.46       |
|    explained_variance   | 0.866       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0211      |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.031      |
|    value_loss           | 0.157       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 30.8        |
|    ep_rew_mean          | 130         |
| time/                   |             |
|    fps                  | 938         |
|    iterations           | 5           |
|    time_elapsed         | 349         |
|    total_timesteps      | 327680      |
| train/                  |             |
|    approx_kl            | 0.027682092 |
|    clip_fraction        | 0.287       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.41       |
|    explained_variance   | 0.87        |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00101    |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0317     |
|    value_loss           | 0.152       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 31.2        |
|    ep_rew_mean          | 129         |
| time/                   |             |
|    fps                  | 927         |
|    iterations           | 6           |
|    time_elapsed         | 424         |
|    total_timesteps      | 393216      |
| train/                  |             |
|    approx_kl            | 0.028157404 |
|    clip_fraction        | 0.289       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.38       |
|    explained_variance   | 0.881       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00475    |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.0319     |
|    value_loss           | 0.136       |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 31.2        |
|    ep_rew_mean          | 131         |
| time/                   |             |
|    fps                  | 920         |
|    iterations           | 7           |
|    time_elapsed         | 498         |
|    total_timesteps      | 458752      |
| train/                  |             |
|    approx_kl            | 0.029764634 |
|    clip_fraction        | 0.296       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.34       |
|    explained_variance   | 0.886       |
|    learning_rate        | 0.0003      |
|    loss                 | 0.0234      |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0329     |
|    value_loss           | 0.128       |
-----------------------------------------
INFO:train_maskablePPO_vecnorm:Entrenando MaskablePPO Fase 3 por 300000 timesteps...
Using cpu device
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 31.2     |
|    ep_rew_mean     | 134      |
| time/              |          |
|    fps             | 1688     |
|    iterations      | 1        |
|    time_elapsed    | 58       |
|    total_timesteps | 98304    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 30.9       |
|    ep_rew_mean          | 132        |
| time/                   |            |
|    fps                  | 1117       |
|    iterations           | 2          |
|    time_elapsed         | 175        |
|    total_timesteps      | 196608     |
| train/                  |            |
|    approx_kl            | 0.03098424 |
|    clip_fraction        | 0.302      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.28      |
|    explained_variance   | 0.886      |
|    learning_rate        | 0.0003     |
|    loss                 | 0.00988    |
|    n_updates            | 10         |
|    policy_gradient_loss | -0.027     |
|    value_loss           | 0.124      |
----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 30.7        |
|    ep_rew_mean          | 136         |
| time/                   |             |
|    fps                  | 1013        |
|    iterations           | 3           |
|    time_elapsed         | 291         |
|    total_timesteps      | 294912      |
| train/                  |             |
|    approx_kl            | 0.030548314 |
|    clip_fraction        | 0.301       |
|    clip_range           | 0.2         |
|    entropy_loss         | -2.25       |
|    explained_variance   | 0.888       |
|    learning_rate        | 0.0003      |
|    loss                 | -0.00854    |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0269     |
|    value_loss           | 0.124       |
-----------------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 30.9       |
|    ep_rew_mean          | 137        |
| time/                   |            |
|    fps                  | 965        |
|    iterations           | 4          |
|    time_elapsed         | 407        |
|    total_timesteps      | 393216     |
| train/                  |            |
|    approx_kl            | 0.03151096 |
|    clip_fraction        | 0.303      |
|    clip_range           | 0.2        |
|    entropy_loss         | -2.22      |
|    explained_variance   | 0.896      |
|    learning_rate        | 0.0003     |
|    loss                 | -0.00567   |
|    n_updates            | 30         |
|    policy_gradient_loss | -0.0273    |
|    value_loss           | 0.118      |
----------------------------------------
INFO:train_maskablePPO_vecnorm:Modelo guardado en best/mppo_ablation/mppo_highhorizon_lowvar/seed_84/best/mppo_yahtzee_vecnorm_final y estadisticas de normalizacion en best/mppo_ablation/mppo_highhorizon_lowvar/seed_84/best/mppo_vecnorm_stats.pkl
INFO:final_evaluate_rl:Autodeteccion: modelo=best/mppo_ablation/mppo_highhorizon_lowvar/seed_84/best/mppo_yahtzee_vecnorm_final, stats=best/mppo_ablation/mppo_highhorizon_lowvar/seed_84/best/mppo_vecnorm_stats.pkl, algo=mppo
INFO:final_evaluate_rl:Cargando modelo: best/mppo_ablation/mppo_highhorizon_lowvar/seed_84/best/mppo_yahtzee_vecnorm_final
INFO:final_evaluate_rl:Algo detectado/forzado: mppo
INFO:final_evaluate_rl:Evaluando el modelo con recompensas SIN normalizar(evaluacion segura)...
INFO:final_evaluate_rl:Episode 1: Recompensa del episodio: 143.78449888690375, Puntuacion final: 146.0
INFO:final_evaluate_rl:Episode 2: Recompensa del episodio: 152.58700222801417, Puntuacion final: 155.0
INFO:final_evaluate_rl:Episode 3: Recompensa del episodio: 189.07399806479225, Puntuacion final: 191.0
INFO:final_evaluate_rl:Episode 4: Recompensa del episodio: 131.88599863369018, Puntuacion final: 135.0
INFO:final_evaluate_rl:Episode 5: Recompensa del episodio: 154.89100132882595, Puntuacion final: 156.0
INFO:final_evaluate_rl:Episode 6: Recompensa del episodio: 108.88999922771472, Puntuacion final: 111.0
INFO:final_evaluate_rl:Episode 7: Recompensa del episodio: 141.39299940038472, Puntuacion final: 143.0
INFO:final_evaluate_rl:Episode 8: Recompensa del episodio: 137.26849834877066, Puntuacion final: 139.0
INFO:final_evaluate_rl:Episode 9: Recompensa del episodio: 134.39450189773925, Puntuacion final: 137.0
INFO:final_evaluate_rl:Episode 10: Recompensa del episodio: 114.89050210017012, Puntuacion final: 117.0
INFO:final_evaluate_rl:Episode 11: Recompensa del episodio: 102.58500149217434, Puntuacion final: 105.0
INFO:final_evaluate_rl:Episode 12: Recompensa del episodio: 164.3549985215068, Puntuacion final: 167.0
INFO:final_evaluate_rl:Episode 13: Recompensa del episodio: 161.55050063529052, Puntuacion final: 163.0
INFO:final_evaluate_rl:Episode 14: Recompensa del episodio: 148.46649891138077, Puntuacion final: 151.0
INFO:final_evaluate_rl:Episode 15: Recompensa del episodio: 124.87650021002628, Puntuacion final: 128.0
INFO:final_evaluate_rl:Episode 16: Recompensa del episodio: 99.98599958233535, Puntuacion final: 102.0
INFO:final_evaluate_rl:Episode 17: Recompensa del episodio: 134.7410011794418, Puntuacion final: 136.0
INFO:final_evaluate_rl:Episode 18: Recompensa del episodio: 102.48049985582475, Puntuacion final: 104.0
INFO:final_evaluate_rl:Episode 19: Recompensa del episodio: 157.87899906234816, Puntuacion final: 162.0
INFO:final_evaluate_rl:Episode 20: Recompensa del episodio: 151.90100031206384, Puntuacion final: 155.0
INFO:final_evaluate_rl:Episode 21: Recompensa del episodio: 196.03049843618646, Puntuacion final: 198.0
INFO:final_evaluate_rl:Episode 22: Recompensa del episodio: 129.47500172932632, Puntuacion final: 131.0
INFO:final_evaluate_rl:Episode 23: Recompensa del episodio: 133.11249928013422, Puntuacion final: 135.0
INFO:final_evaluate_rl:Episode 24: Recompensa del episodio: 79.38749955047388, Puntuacion final: 81.0
INFO:final_evaluate_rl:Episode 25: Recompensa del episodio: 120.10199903394096, Puntuacion final: 122.0
INFO:final_evaluate_rl:Episode 26: Recompensa del episodio: 169.04150046606082, Puntuacion final: 171.0
INFO:final_evaluate_rl:Episode 27: Recompensa del episodio: 84.39949878142215, Puntuacion final: 86.0
INFO:final_evaluate_rl:Episode 28: Recompensa del episodio: 159.23099900531815, Puntuacion final: 161.0
INFO:final_evaluate_rl:Episode 29: Recompensa del episodio: 107.1045018961886, Puntuacion final: 109.0
INFO:final_evaluate_rl:Episode 30: Recompensa del episodio: 134.40250086504966, Puntuacion final: 137.0
INFO:final_evaluate_rl:Episode 31: Recompensa del episodio: 172.3700001038378, Puntuacion final: 174.0
INFO:final_evaluate_rl:Episode 32: Recompensa del episodio: 111.06899938732386, Puntuacion final: 113.0
INFO:final_evaluate_rl:Episode 33: Recompensa del episodio: 142.49049896467477, Puntuacion final: 145.0
INFO:final_evaluate_rl:Episode 34: Recompensa del episodio: 155.54349932447076, Puntuacion final: 158.0
INFO:final_evaluate_rl:Episode 35: Recompensa del episodio: 148.67149801296182, Puntuacion final: 151.0
INFO:final_evaluate_rl:Episode 36: Recompensa del episodio: 146.3724989669863, Puntuacion final: 148.0
INFO:final_evaluate_rl:Episode 37: Recompensa del episodio: 116.88549979031086, Puntuacion final: 119.0
INFO:final_evaluate_rl:Episode 38: Recompensa del episodio: 185.0650017753942, Puntuacion final: 188.0
INFO:final_evaluate_rl:Episode 39: Recompensa del episodio: 169.34499977581436, Puntuacion final: 171.0
INFO:final_evaluate_rl:Episode 40: Recompensa del episodio: 113.89799845940433, Puntuacion final: 115.0
INFO:final_evaluate_rl:Episode 41: Recompensa del episodio: 117.26200055936351, Puntuacion final: 119.0
INFO:final_evaluate_rl:Episode 42: Recompensa del episodio: 151.90500151365995, Puntuacion final: 156.0
INFO:final_evaluate_rl:Episode 43: Recompensa del episodio: 163.3529988649534, Puntuacion final: 165.0
INFO:final_evaluate_rl:Episode 44: Recompensa del episodio: 85.87349857483059, Puntuacion final: 89.0
INFO:final_evaluate_rl:Episode 45: Recompensa del episodio: 146.92149983951822, Puntuacion final: 149.0
INFO:final_evaluate_rl:Episode 46: Recompensa del episodio: 158.17399956053123, Puntuacion final: 160.0
INFO:final_evaluate_rl:Episode 47: Recompensa del episodio: 169.36300058860797, Puntuacion final: 172.0
INFO:final_evaluate_rl:Episode 48: Recompensa del episodio: 87.76599899819121, Puntuacion final: 90.0
INFO:final_evaluate_rl:Episode 49: Recompensa del episodio: 169.0594999223249, Puntuacion final: 171.0
INFO:final_evaluate_rl:Episode 50: Recompensa del episodio: 125.18400091631338, Puntuacion final: 127.0
INFO:final_evaluate_rl:Metricas guardadas en mppo_vecnorm_final_evaluation_metrics.txt
Grafica guardada en best/mppo_ablation/mppo_highhorizon_lowvar/seed_84/best/mppo_vecnorm_reward_evolution.png

Resumen:
  Recompensa media (sin normalizar): 138.1348
  Puntuacion final promedio:        140.2800
